# AP2 Report.md

## 1. Abstract

This report details the development of a Deep Reinforcement Learning (DRL) agent capable of playing Super Mario Bros. The primary objective was to train an agent to navigate game environments effectively, demonstrating proficiency in movement, obstacle avoidance, and level completion. The methodology involved implementing and comparing various DRL algorithms, specifically Deep Q-Networks (DQN), Double DQN (DDQN), and Dueling DQN, utilizing the PyTorch framework. Key components include custom environment wrappers for frame preprocessing and reward shaping, a robust neural network architecture, and an experience replay buffer. The project also focused on developing a comprehensive training pipeline with advanced logging, visualization, and hyperparameter optimization using a genetic algorithm. Findings indicate that iterative reward shaping and environment modifications were crucial for overcoming agent stagnation and improving learning efficiency. The project successfully demonstrates the application of DRL techniques to complex game environments, offering insights into agent behavior and training dynamics.

## 2. Introduction

Reinforcement Learning (RL) has emerged as a powerful paradigm for developing intelligent agents capable of learning optimal behaviors through interaction with dynamic environments. This academic project focuses on applying Deep Reinforcement Learning (DRL) to the classic Nintendo game, Super Mario Bros., a challenging domain due to its complex state space, sparse rewards, and real-time dynamics. The report outlines the design, implementation, and evaluation of a DRL agent, specifically exploring variations of Q-learning algorithms.

The structure of this report is organized into eleven sections, beginning with this introduction and an abstract, followed by a detailed exposition of project objectives, a comprehensive literature review, and a precise problem statement. Subsequent sections delve into the research and development methodology, system analysis and design, and the technical aspects of system development. The report concludes with an evaluation of the system's performance, a summary of key findings, and a list of references.

Existing works in DRL have shown promising results in various game environments, from Atari games to more complex simulations. However, challenges remain in achieving robust and generalizable performance, particularly in environments like Super Mario Bros. where precise timing and strategic decision-making are paramount. This project aims to bridge some of these gaps by implementing advanced DRL architectures and employing iterative reward shaping techniques. The scope of this project encompasses the development of a DRL agent, its training, evaluation, and the analysis of its learned behaviors. The project schedule, as initially planned, involved a six-week timeline covering environment setup, agent architecture, training pipeline, and final evaluation.

## 3. Project Objectives

### To use deep reinforcement learning to achieve human-level performance but in more complex multi-agent games

This project aims to push the boundaries of Deep Reinforcement Learning by developing an agent capable of achieving human-level performance not just in single-agent environments, but with an eye towards the complexities inherent in multi-agent scenarios. While the current focus is on a single-player game, the underlying principles and architectural choices, such as robust environment interaction and sophisticated reward mechanisms, are designed to lay a foundation for future expansion into more intricate, interactive environments where multiple intelligent entities operate. The goal is to demonstrate that DRL can effectively learn complex behaviors that rival human intuition and skill in dynamic, unpredictable settings.

### To design a reward shaping system and custom environment wrappers to overcome complex, emergent behavioral flaws in a Deep Reinforcement Learning agent

A significant objective of this project is to address the common challenge of emergent behavioral flaws in DRL agents, such as stagnation, repetitive actions, or an inability to perform specific critical maneuvers. This is achieved through the meticulous design and iterative refinement of a custom reward shaping system. This system provides dense, informative feedback to the agent, guiding its learning beyond sparse in-game rewards. Complementing this, custom environment wrappers are developed to preprocess observations and modify action execution, ensuring the agent receives optimal input and can perform necessary actions, thereby overcoming complex learning hurdles and fostering more intelligent, adaptive behaviors.

### To develop a behavior analysis framework that can identify, categorize, and document emergent strategic behaviors in AI agents

Beyond simply training an agent, a core objective is to understand *how* the agent learns and *what* strategies it develops. This project includes the development of a comprehensive behavior analysis framework. This framework is designed to systematically identify, categorize, and document the emergent strategic behaviors exhibited by the AI agent during its learning process. By tracking milestones, analyzing gameplay recordings, and correlating actions with performance metrics, this framework provides crucial insights into the agent's decision-making, allowing for a deeper understanding of DRL's capabilities and limitations in complex environments.

## 4. Literature Review

### Proceedings/Journal Articles & Related Topics

Deep Reinforcement Learning (DRL) has seen significant advancements, particularly with the advent of Deep Q-Networks (DQN) by Mnih et al. (2013, 2015). This foundational work demonstrated that a convolutional neural network could learn to play Atari games directly from pixel inputs, achieving human-level performance on many titles. Key innovations included experience replay, which breaks correlations in the observation sequence, and a target network, which stabilizes training.

Subsequent research addressed limitations of DQN. Van Hasselt et al. (2016) introduced **Double DQN (DDQN)** to mitigate the overestimation bias inherent in standard DQN, where the maximum Q-value is used for both action selection and evaluation. DDQN decouples these two steps, leading to more stable and reliable learning. Wang et al. (2016) proposed **Dueling Network Architectures for Deep Reinforcement Learning**, which separates the estimation of state-value and advantage functions. This architecture allows the network to learn the value of states independently of the actions, improving the efficiency of learning, especially in environments where many actions do not affect the environment in a meaningful way.

Further advancements in DRL for game playing include techniques like **Prioritized Experience Replay** (Schaul et al., 2015), which samples important transitions more frequently, and various **Reward Shaping** strategies (Ng et al., 1999) to guide agents in sparse reward environments. The application of DRL to classic Nintendo games like Super Mario Bros. presents unique challenges due to the game's complexity, requiring robust environment interaction and effective reward design.

### Existing Products/Systems (Product Comparison)

This project, focused on training a Deep Reinforcement Learning agent for Super Mario Bros., shares conceptual similarities with broader platforms designed for AI and robotics simulation, such as NVIDIA Omniverse and Unity's ML-Agents. However, significant differences exist in their scope, design philosophy, and target applications.

NVIDIA Omniverse is a powerful, extensible platform for 3D design collaboration and simulation, built on Universal Scene Description (USD). Its strength lies in creating highly realistic, physically accurate virtual environments for training robots, autonomous vehicles, and other AI systems. Omniverse provides a rich ecosystem for synthetic data generation and large-scale simulation, allowing for complex interactions and high-fidelity sensor data. The design goal of Omniverse is to provide a comprehensive, scalable platform for industrial and scientific simulation, emphasizing photorealism and physics. In contrast, this Super Mario Bros. project operates within a much simpler, 2D, pixel-based environment. While both aim to train AI agents, Omniverse focuses on complex, real-world-like scenarios with high computational demands for rendering and physics, whereas this project prioritizes the algorithmic challenges of DRL within a classic game, focusing on efficient learning from abstract game states and overcoming sparse rewards. The project's environment is a direct interface to the game, not a simulated world built for general-purpose AI training.

Similarly, Unity's ML-Agents Toolkit provides an open-source platform for training intelligent agents in Unity games and simulations using reinforcement learning, imitation learning, and other AI methods. ML-Agents offers a flexible framework for defining observation and action spaces, integrating various RL algorithms, and scaling training across multiple environments. Its design goal is to empower game developers and AI researchers to create intelligent behaviors within Unity's interactive 3D environments. This project shares the goal of training agents in a game environment and utilizes similar DRL algorithms. However, ML-Agents is a general-purpose toolkit for Unity's diverse game ecosystem, providing abstractions for various game types and complex 3D physics. This Super Mario Bros. project is highly specialized for a single, specific 2D game, with custom environment wrappers and reward shaping tailored precisely to its unique mechanics and challenges. While ML-Agents provides a broad toolkit, this project delves deeper into the specific nuances of a retro game environment, focusing on fine-grained reward engineering and behavioral analysis that might be abstracted away in a more general toolkit.

### Related Applicable Theories/Methods/Frameworks/Models & Algorithms

Reinforcement Learning (RL) serves as the fundamental theoretical framework for this project. It models the problem as an agent learning to make sequential decisions within an environment to maximize a cumulative reward signal. This learning process is typically formalized as a Markov Decision Process (MDP), which defines the states, actions, transition probabilities, and rewards within the environment.

Deep Learning (DL) is integrated into the RL framework to handle the high-dimensional observations (pixel data) from the game. Deep neural networks are employed to approximate the Q-value function, which estimates the expected future reward for taking a specific action in a given state. This combination forms the basis of Deep Reinforcement Learning (DRL).

The core algorithm utilized is Q-Learning, a model-free RL algorithm that learns an action-value function. This project specifically implements Deep Q-Networks (DQN), which combine Q-learning with deep neural networks. Key innovations in DQN, such as Experience Replay, are central to the agent's learning process. Experience replay involves storing past interactions (state, action, reward, next state, done) in a buffer and then randomly sampling mini-batches from this buffer for training. This technique breaks the temporal correlations in the observation sequence, which is crucial for stabilizing the training of deep neural networks in RL. Additionally, DQN utilizes a Target Network, a separate, delayed copy of the main Q-network, to provide stable targets for the Q-value updates, further enhancing training stability.

To address the overestimation bias often present in standard DQN, Double Deep Q-Networks (DDQN) are also implemented. DDQN improves stability and performance by decoupling the selection of the next action from the evaluation of its Q-value, using the main network for action selection and the target network for value estimation. Furthermore, the project incorporates Dueling Deep Q-Networks, a specialized neural network architecture. This architecture explicitly separates the estimation of state-value and advantage functions. This design allows the network to learn the value of states independently of the actions, which can significantly improve learning efficiency, especially in environments where many actions do not affect the environment in a meaningful way.

Convolutional Neural Networks (CNNs) are used as the primary feature extractor for processing the raw pixel input from the game environment. These networks are adept at identifying spatial hierarchies of features, which is crucial for understanding the visual state of the game. Experience Replay, as mentioned, is a vital mechanism that stores past interactions and samples them randomly for training, thereby improving learning stability and data efficiency.

Reward Shaping is a critical technique employed to guide the agent's learning process in environments with sparse natural rewards. By designing additional reward signals (e.g., for forward progress, avoiding stagnation, performing specific jumps), the agent receives more frequent and informative feedback, accelerating the learning of desired behaviors. Finally, Genetic Algorithms (GA) are utilized as an optimization technique for hyperparameter tuning. Inspired by natural selection, GAs evolve a population of hyperparameter sets, evaluating their performance (fitness) and selecting the best ones to generate new, potentially more optimal configurations.

## 5. Problem Statement

**Ideal Situation:** An ideal Super Mario Bros. AI agent would autonomously navigate diverse levels, overcome obstacles, defeat enemies, collect power-ups, and consistently reach the flagpole, demonstrating intelligent and adaptive gameplay without explicit programming for each scenario. Such an agent would learn optimal strategies purely through interaction with the game environment, maximizing its score and minimizing completion time.

**Current Problem:** Training a DRL agent for Super Mario Bros. presents several significant challenges. The game's reward structure is often sparse, with positive rewards primarily occurring at the end of a level or upon collecting specific items, making it difficult for the agent to learn intermediate behaviors. Agents frequently exhibit local optima behaviors, such as getting stuck in loops, failing to perform necessary high jumps, or exploiting minor reward signals (e.g., moving right without actual progress). The high-dimensional pixel input and real-time dynamics further complicate the learning process, often leading to slow convergence and suboptimal policies. Debugging and understanding the agent's learning process are also challenging due to the black-box nature of deep neural networks.

**Proposed Solution:** This project proposes a comprehensive DRL solution leveraging advanced Q-learning variants (DQN, DDQN, Dueling DQN) within a PyTorch framework. The core of the solution involves:
1.  **Sophisticated Environment Wrappers:** Implementing custom wrappers for efficient frame preprocessing (grayscale, resize, frame stacking, frame skipping) and, critically, iterative reward shaping to provide dense and informative feedback, addressing issues like stagnation, inefficient movement, and inability to clear obstacles (e.g., a wrapper to ensure prolonged jumps).
2.  **Modular Agent Architecture:** Designing a flexible agent structure that allows for easy experimentation and comparison between different DRL algorithms.
3.  **Enhanced Training Workflow:** Developing a robust training pipeline with decoupled hyperparameter search (using a Genetic Algorithm), persistent hyperparameter saving, and automated evaluation.
4.  **Comprehensive Logging and Visualization:** Integrating detailed metrics tracking and visual recording of agent gameplay to provide transparency into the learning process, facilitate debugging, and enable qualitative analysis of learned behaviors.

By combining these elements, the project aims to develop a more intelligent and robust Super Mario Bros. DRL agent, capable of overcoming common training pitfalls and demonstrating more human-like adaptive gameplay.

## 6. Research/Development Methodology

The research and development methodology adopted for this academic project followed an iterative and modular approach, primarily driven by the principles of Deep Reinforcement Learning and software engineering best practices.

**Design of Research and/or Development Methodology:**

### Problem Definition and Planning

The initial phase involved a thorough analysis of the user's request to train a Deep Reinforcement Learning agent for Super Mario Bros. This led to the development of a comprehensive project plan, which encompassed the detailed setup of the environment, the design of the agent's architecture, the establishment of a robust training pipeline, and a clear strategy for evaluation. The plan was iteratively refined based on feedback, incorporating specific DRL algorithms such as DQN, Double DQN, and Dueling DQN, alongside requirements for extensive logging and visualization. Furthermore, the academic project requirements, including the detailed report structure and measurable objectives, were integrated into this foundational planning stage, ensuring alignment with the project's academic goals.

### Project Scaffolding and Modularization

Following the planning phase, the project scaffolding was established by creating a well-defined directory structure. This included dedicated subdirectories for agents, environment components, neural network models, utility functions, and configuration files. The implementation of empty Python files within these modules at the outset provided a clear development roadmap and enforced a modular design. This approach significantly facilitated parallel development efforts and simplified the debugging process, as each component could be developed and tested in isolation before integration into the larger system.

### Iterative Development Cycles (Phases)

The development process was structured into distinct, iterative phases, each building upon the previous one. The first phase focused on environment setup and testing, ensuring the game was playable and observable by the agent, including the development of essential environment wrappers. The second phase concentrated on building the core components of the DRL agent, such as neural network models and the experience replay buffer, followed by the specific agent implementations. The third phase involved developing the central training pipeline, integrating command-line arguments, the main training loop, and comprehensive logging and visualization systems. The final phase focused on assessing the trained agents and providing tools for interaction and deployment. Throughout all these phases, a continuous refinement process was applied, particularly for reward shaping and training workflow, involving analysis of agent behavior, identification of issues, proposal of solutions, implementation of changes, and re-evaluation. This iterative feedback loop was crucial for improving agent performance and robustness.

**Appropriateness of Data Gathering Techniques:**

The data gathering techniques employed throughout this project were carefully selected to provide a rich and diverse dataset for both quantitative analysis of training dynamics and qualitative understanding of agent behavior, ensuring that the development process was data-driven and effectively addressed the project objectives. The experience replay mechanism served as the primary data gathering method for agent training, storing and sampling past transitions to break temporal correlations and stabilize learning. The logging system systematically collected quantitative data, including per-episode metrics and training metrics, which were crucial for monitoring progress and debugging. The visualization tools provided qualitative data through recorded gameplay videos, allowing for direct observation of agent behavior and learning progress. A behavior tracker gathered specific behavioral data, such as reaching new X-positions or completing levels, providing discrete indicators of learning progress. Finally, the genetic algorithm acted as an automated data gathering technique for optimal hyperparameters, systematically exploring the hyperparameter space and saving the best-found parameters, thereby reducing manual tuning effort and improving overall development efficiency.

## 7. System Analysis and Design

### Detailed Requirements (Functional and Non-Functional)

**Functional Requirements:**

The system must be capable of interacting with the Super Mario Bros. game environment, receiving observations in the form of pixel frames and sending corresponding game actions. It is required to preprocess these raw game frames by converting them to grayscale, resizing them to a standardized 84x84 pixels, and stacking multiple frames (e.g., four) to capture essential temporal information for the agent. The system must also effectively map a simplified set of agent actions to the comprehensive range of Super Mario Bros. controls. A custom reward function is essential to provide dense feedback to the agent, incorporating positive rewards for forward progress, penalties for idling or moving left, and bonuses for specific in-game events such as collecting coins, defeating enemies, acquiring power-ups, and successfully completing levels. An experience replay buffer must be maintained to store and randomly sample past transitions, which is crucial for stable training. The system needs to implement a Convolutional Neural Network (CNN) as a feature extractor and support both standard Deep Q-Network (DQN) and Dueling DQN architectures for estimating Q-values. Distinct implementations for DQN, Double DQN, and Dueling DQN agents are required, each capable of epsilon-greedy action selection and learning from collected experiences. A robust training loop must be included to iteratively interact with the environment, collect experiences, update agent models, and perform periodic evaluations. The system must also support model checkpointing, allowing for saving and loading trained agent models at specified intervals. Comprehensive logging capabilities are necessary to record various metrics, including episode rewards, lengths, maximum X-positions reached, loss values, Q-value statistics, and exploration rates, to persistent storage for analysis. Visualization functionality is required to render and record agent gameplay, with the ability to overlay real-time metrics onto the video. Furthermore, the system must support hyperparameter optimization, ideally through a genetic algorithm, to efficiently discover optimal training configurations. Finally, the system must allow for playback of trained agents, enabling observation of their gameplay with an optional human takeover feature, and provide an evaluation capability to assess a trained agent's performance across multiple episodes and levels, generating statistical summaries.

**Non-Functional Requirements:**

The system's performance is critical, requiring the training process to be computationally efficient, ideally leveraging GPU acceleration. Agent inference during live gameplay must operate in real-time to ensure a smooth experience. Modularity is a key design principle, ensuring the codebase is highly modular to allow for easy swapping of components such as different agent algorithms, reward functions, or network architectures. Extensibility is also paramount, enabling the system to be easily expanded to incorporate new Deep Reinforcement Learning algorithms, environment wrappers, or logging features in the future. Maintainability is addressed by ensuring the code is well-structured, thoroughly documented, and adheres to Python best practices. For usability, particularly for researchers and developers, the training and evaluation scripts should be configurable via command-line arguments, providing flexibility without necessitating direct code modifications. The system must demonstrate robustness by handling common errors gracefully, such as environment resets or invalid inputs, and providing clear error messages. Finally, reproducibility is a crucial requirement, ensuring that training runs can be consistently replicated given the same initial conditions and hyperparameters.

### Complete Interface Design (Story Board, Mock-up)

While a graphical storyboard or mock-up is not directly applicable to a command-line driven DRL project, the "interface" primarily refers to:

1.  **Command-Line Interface (CLI):**
    *   The training script provides arguments to select the agent type (DQN, DDQN, or Dueling), specify a configuration file for hyperparameters, set the total number of training steps, initiate a hyperparameter search with a defined number of generations, load pre-trained models, and assign a unique label for organizing output files.
    *   The evaluation script accepts arguments to specify the path to a trained model, the agent type, the number of episodes for evaluation, and whether to record a GIF of the gameplay.
    *   A separate script for playing a trained agent allows loading a model and observing its performance, with an optional feature for human intervention.
    *   A simple script is also available for human testing of the environment, ensuring basic functionality.

2.  **Visual Output (Recordings & TensorBoard):**
    *   **GIF/MP4 Recordings:** The Visualizer generates video files of agent gameplay, often with overlaid metrics (episode number, total reward, max X-position). This provides a direct visual interface to observe agent behavior and learning progress.
    *   **TensorBoard Dashboard:** The Logger integrates with TensorBoard, providing a web-based dashboard for visualizing quantitative metrics over time (e.g., reward curves, loss plots, Q-value distributions, epsilon decay). This serves as the primary analytical interface for training dynamics.

### Complete System Design (e.g., UML diagram, HTA, AI techniques)

**High-Level System Architecture (Modular Design):**

The system is designed with a clear separation of concerns, organized into distinct modules, as illustrated below:

```
+-----------------+
|    User/CLI     |
+-----------------+
        |
        V
+-----------------------+
|   Training Orchestrator |
+-----------------------+
        |
        V
+-----------------+     +-----------------+
|      Agent      |---->|   Environment   |
+-----------------+     +-----------------+
        ^                       |
        |                       |
+-----------------+     +-----------------+
|      Logger     |<----|    Visualizer   |
+-----------------+     +-----------------+
        ^                       |
        |                       |
+-----------------------+       V
| Genetic Algorithm     |----->| Configuration |
+-----------------------+       +---------------+
        |
        V
+-----------------+
|  Trained Models |
+-----------------+
```

**Key AI Techniques and Components:**

The project's foundation lies in **Reinforcement Learning (RL)**, which is the theoretical framework guiding the agent's learning process. Within this framework, the problem is modeled as a **Markov Decision Process (MDP)**, defining the sequential decision-making problem in terms of states, actions, transitions, and rewards.

**Deep Learning (DL)** is extensively used to handle the high-dimensional visual observations from the game. Deep neural networks are employed to approximate the complex Q-value function, which estimates the expected future reward for taking a specific action in a given state. This combination forms the basis of the **Deep Reinforcement Learning (DRL)** approach.

The primary learning algorithm utilized is **Q-Learning**, a model-free RL algorithm that learns an action-value function. This project specifically implements **Deep Q-Networks (DQN)**, which combine Q-learning with deep neural networks. Key innovations in DQN, such as **Experience Replay**, are central to the agent's learning process. Experience replay involves storing past interactions (state, action, reward, next state, done) in a buffer and then randomly sampling mini-batches from this buffer for training. This technique breaks the temporal correlations in the observation sequence, which is crucial for stabilizing the training of deep neural networks in RL. Additionally, DQN utilizes a **Target Network**, a separate, delayed copy of the main Q-network, to provide stable targets for the Q-value updates, further enhancing training stability.

To address the overestimation bias often present in standard DQN, **Double Deep Q-Networks (DDQN)** are also implemented. DDQN improves stability and performance by decoupling the selection of the next action from the evaluation of its Q-value, using the main network for action selection and the target network for value estimation. Furthermore, the project incorporates **Dueling Deep Q-Networks**, a specialized neural network architecture. This architecture explicitly separates the estimation of state-value and advantage functions. This design allows the network to learn the value of states independently of the actions, which can significantly improve learning efficiency, especially in environments where many actions do not affect the environment in a meaningful way.

**Convolutional Neural Networks (CNNs)** are used as the primary feature extractor for processing the raw pixel input from the game environment. These networks are adept at identifying spatial hierarchies of features, which is crucial for understanding the visual state of the game. **Experience Replay**, as mentioned, is a vital mechanism that stores past interactions and samples them randomly for training, thereby improving learning stability and data efficiency.

**Reward Shaping** is a critical technique employed to guide the agent's learning process in environments with sparse natural rewards. By designing additional reward signals (e.g., for forward progress, avoiding stagnation, performing specific jumps), the agent receives more frequent and informative feedback, accelerating the learning of desired behaviors. Finally, **Genetic Algorithms (GA)** are utilized as an optimization technique for hyperparameter tuning. Inspired by natural selection, GAs evolve a population of hyperparameter sets, evaluating their performance (fitness) and selecting the best ones to generate new, potentially more optimal configurations.

## 8. System Development

The system development phase involved the technical implementation of all proposed modules, focusing on robust code, user interface/experience considerations (primarily through visualization and logging), and managing system complexity.

### Technical Implementation of the Proposed Modules

1.  **Project Scaffolding:** The initial step in development involved establishing a well-organized project structure. This included creating a main directory for the Super Mario Bros. Reinforcement Learning project, along with dedicated subdirectories for agents, environment components, neural network models, utility functions, and configuration files. This structured approach was crucial for facilitating organized development, promoting clear separation of concerns among different parts of the system, and laying a solid foundation for the entire project.

2.  **Environment Setup and Wrappers:** The environment setup began with implementing a human playable test to verify basic game functionality and human control, ensuring the underlying game environment was correctly integrated. Core environment wrappers, such as those for grayscale observation, observation resizing, and frame stacking, were developed to preprocess raw game observations into a format suitable for neural network input. A frame skipping mechanism was added to enhance computational efficiency by repeating actions over multiple game frames. Crucially, a reward shaping system was iteratively refined to provide dense and informative feedback to the agent. This system evolved from simple rightward movement rewards to include penalties for idling or moving left, a "stuck" detector that rewarded jumps when the agent was unable to progress, and significant scaling of rewards to amplify the learning signal. A particularly important development was a wrapper designed to ensure Mario performs full-height jumps, which involved a stateful implementation to resolve conflicts with other wrappers and prevent environment errors.

3.  **Neural Network Models:** The PyTorch deep learning framework was selected and installed for implementing the neural network models. A base Convolutional Neural Network (CNN) was developed as the primary feature extractor, comprising multiple convolutional layers followed by ReLU activations and a linear layer to process the game frames. Building upon this base, a Dueling DQN architecture was implemented. This design takes the features extracted by the base CNN and splits them into two distinct streams: one for estimating the state-value function and another for estimating the advantage function for each action. These two streams are then combined to produce the final Q-values, enhancing the network's ability to learn effectively.

4.  **Replay Buffer:** A replay buffer component was implemented as a fixed-size circular buffer. Its primary function is to store past experiences, specifically tuples containing the agent's state, action, reward, next state, and a done flag. This buffer is capable of providing random batches of these stored experiences for training the neural networks. This mechanism is vital for breaking temporal correlations in the data, which significantly stabilizes and improves the efficiency of the deep reinforcement learning process.

5.  **Agent Implementations:** A base agent class was developed to encapsulate common functionalities shared across all reinforcement learning agents, including epsilon-greedy action selection for exploration, efficient experience collection, and robust mechanisms for saving and loading trained models. Building upon this base, a Deep Q-Network (DQN) agent was implemented, incorporating the standard DQN algorithm with a target network to stabilize the learning process. A Double DQN (DDQN) agent was also developed, which refines the DQN algorithm by decoupling the action selection from the action evaluation, thereby reducing overestimation bias in Q-value estimates. Finally, a Dueling agent was implemented as a factory function, allowing the integration of the Dueling network architecture with either the DQN or DDQN algorithms, providing flexibility in agent design and experimentation.

6.  **Training Pipeline:** The central training script was developed to orchestrate the entire learning process. This script includes robust command-line argument parsing, enabling flexible configuration of hyperparameters and training settings. It manages the main training loop, which iteratively interacts with the environment, collects experiences, and updates the agent's models. The pipeline also incorporates model checkpointing, allowing for the periodic saving of trained models, and schedules for logging and visualization. Advanced workflow features were integrated, such as a flag to initiate hyperparameter search using the genetic algorithm, flexible argument parsing for loading pre-trained models and selecting specific hyperparameter configurations, and automated saving of the final trained model followed by an immediate launch of the evaluation script. Furthermore, epsilon decay and evaluation intervals were dynamically scaled based on the total number of training steps, ensuring consistent learning trajectories across different training durations.

7.  **Logging and Visualization:** Comprehensive logging and visualization capabilities were integrated to monitor and understand the agent's learning progress. A logger component was implemented to track a wide array of per-episode and training metrics, including total rewards, episode lengths, maximum X-positions reached, loss values, Q-value statistics, and exploration rates. This data is integrated with TensorBoard for graphical visualization and also saved to text files for raw data analysis. Rolling statistics were added to provide smoothed performance indicators, making trends easier to discern. A visualizer component was developed to render and record agent gameplay. This component was carefully refactored to create an isolated environment instance specifically for recording, preventing any interference with the main training loop, and ensuring that recordings are generated only at the completion of an episode. Text overlays displaying real-time metrics were added to the recordings, providing immediate context to the agent's performance. Additionally, a behavior tracker is implemented to track and log specific learning milestones and emergent strategic behaviors, offering qualitative insights into the agent's learning process.

8.  **Genetic Algorithm:** A genetic algorithm component was implemented to automate the process of hyperparameter optimization. This algorithm manages populations of hyperparameters, evaluates their fitness based on agent performance, and generates new generations through processes inspired by natural selection (selection, crossover, mutation). The best-found hyperparameters from each generation are persistently saved to timestamped JSON files, ensuring that the results of long optimization runs are not lost and can be tracked over time. This automation significantly reduces the manual effort traditionally associated with hyperparameter tuning and improves the overall efficiency of finding optimal configurations.

9.  **Evaluation and Deployment:** Dedicated scripts were developed for evaluating and deploying trained agents. The evaluation script was enhanced to run multiple evaluation episodes, typically ten, using an epsilon-greedy policy with a small exploration rate (e.g., ε=0.05) to provide more reliable and stable performance statistics. This script correctly utilizes the existing wrapped environment passed from the training loop, preventing environment re-initialization issues. It generates comprehensive performance statistics, including average reward and maximum X-position, and supports saliency map visualization to understand which parts of the input the agent focuses on. A separate agent playback script was developed to load and run trained agents, displaying real-time performance metrics during gameplay. This script also includes a human takeover functionality, allowing a user to manually control Mario during an agent's run, which is useful for debugging and interactive analysis.

### User Interface and/or User Experience

The "user interface" for this project is primarily through the command-line and the generated outputs:

The **Command-Line Interface (CLI)** serves as the primary means for users to interact with the system. Scripts such as the training, evaluation, and agent playback scripts are designed with clear, intuitive command-line arguments. This design choice significantly enhances usability for researchers and developers, allowing them to configure and run experiments with flexibility without needing to modify the underlying source code. This approach promotes a streamlined workflow and makes the system more accessible for various experimental setups.

For visual feedback, the **Visualizer** component generates GIF or MP4 recordings of the agent's gameplay. These visual outputs are invaluable for understanding the agent's learned behaviors, identifying any unexpected issues, and qualitatively assessing its progress over time. The quality of interaction is further enhanced by overlaying real-time metrics, such as the episode number, total reward, and maximum X-position, directly onto the video. This provides immediate context and allows for a direct correlation between the agent's actions and its performance.

Quantitative feedback is primarily provided through **TensorBoard** integration via the Logger component. TensorBoard offers an interactive, web-based dashboard that allows users to visualize quantitative metrics over time. This includes reward curves, loss plots, Q-value distributions, and epsilon decay rates. This rich graphical interface provides a comprehensive user experience for analyzing training dynamics and making data-driven decisions about the agent's learning process. Additionally, console logs provide real-time textual updates on training progress, offering immediate insights during active runs.

While not a traditional graphical user interface, the clear separation of concerns and well-defined APIs within the Python modules contribute significantly to the **accessibility** of the codebase. This modularity makes it easier for other developers to understand, extend, and contribute to the project, fostering collaborative development and future research.

### System Complexity

The system's complexity stems from several interconnected factors inherent in developing a sophisticated Deep Reinforcement Learning agent for a complex game environment.

One major factor is the **integration of multiple DRL algorithms**. Implementing and ensuring compatibility between Deep Q-Networks (DQN), Double DQN (DDQN), and Dueling DQN, each with its own specific nuances such as target networks and decoupled updates, adds a significant layer of algorithmic complexity. Each algorithm requires careful implementation to ensure correct behavior and interaction within the overall training pipeline.

**Environment engineering** introduces another substantial source of complexity. The development of custom environment wrappers, particularly the reward shaping system and the prolonged jump wrapper, is intricate due to their stateful nature and the critical need for precise synchronization with the base environment and other wrappers. Debugging conflicts between these wrappers, such as the recurring "cannot step in a done environment!" error, demanded a deep understanding of the Gym environment API and careful architectural design to resolve.

The process of **iterative reward shaping** itself is highly complex. It requires continuous analysis of the agent's emergent behaviors and precise, often subtle, adjustments to the reward signals. This iterative refinement is challenging because poorly designed rewards can lead to unintended consequences, such as the agent exploiting minor reward signals without making genuine progress, or getting stuck in local optima.

**Training workflow management** adds further complexity. Decoupling the hyperparameter search from the main training loop, managing the persistence of optimal hyperparameters, and automating the evaluation process introduces layers of intricate logic within the main training script. This requires careful orchestration to ensure a smooth and efficient experimental pipeline.

**Logging and visualization overheads** also contribute to the system's complexity. Integrating comprehensive logging and video recording capabilities without interfering with the core training loop demanded careful design. A key solution involved isolating the visualizer's environment instance to prevent desynchronization and ensure that recording processes do not negatively impact the agent's learning.

Finally, **hyperparameter optimization** using a genetic algorithm is a complex optimization technique in itself. It requires careful design of fitness functions, selection mechanisms, crossover operations, and mutation strategies to effectively explore the vast hyperparameter space and find optimal configurations.

Despite these inherent complexities, the project's modular design, clear project structure, and iterative development approach were instrumental in managing the overall system complexity. This allowed for focused development and debugging of individual components, contributing to a robust and maintainable system.

## 9. System Evaluation

System evaluation focused on both the technical correctness of the implementation and the performance of the trained agents. This involved rigorous testing techniques and ensuring robust error handling.

### Testing Techniques

1.  **Unit Testing:** Individual components were tested in isolation to verify their correct functionality. For environment wrappers, such as those for grayscale observation, resizing, frame stacking, frame skipping, reward shaping, and prolonged jumps, tests ensured they accurately transformed observations, managed frame progression, and applied rewards as intended. For instance, a dedicated script was created to immediately test recording functionality, which implicitly validated the visualizer and certain environment interactions. Neural network models underwent tests to confirm correct output shapes and basic operations with dummy inputs. The replay buffer was tested for accurate storage, sampling, and adherence to capacity limits. Individual methods within the base agent and specific agent implementations (DQN, Double DQN, Dueling) were tested to ensure their logic was sound.

2.  **Integration Testing:** This phase focused on verifying that different modules functioned correctly when combined. The full stack of environment wrappers was tested together to ensure seamless operation without conflicts, with the resolution of a persistent environment error due to wrapper interaction being a key success. The main training script served as a primary integration test, confirming that the agent could interact with the wrapped environment, collect experiences, and update its model effectively. Furthermore, the integration of the logging and visualization systems was tested to ensure that metrics were correctly captured and displayed in TensorBoard, and that gameplay recordings did not disrupt the training process.

3.  **System Testing:** Comprehensive end-to-end training runs were conducted using the main training script with various agent types and hyperparameter configurations. These runs assessed the overall system stability, performance, and learning capabilities. A dedicated evaluation script was used to perform thorough system-level assessments of trained models, running the agent through multiple episodes (e.g., 10) with a small exploration rate to provide more reliable performance statistics such as average reward and maximum X-position. This also validated the model loading and environment setup within an evaluation context. An agent playback script allowed for visual inspection of the trained agent's behavior in real-time, offering a qualitative assessment of its learned policy. The human takeover functionality within this script also served as a form of interactive system testing. Finally, the effectiveness of the hyperparameter optimization using the genetic algorithm was evaluated by comparing the performance of agents trained with GA-optimized settings against those with manually tuned parameters.

4.  **User Acceptance Test (UAT):** While formal UAT with external users was not conducted, the iterative feedback loop with the project's "user" (the prompt's instructions and subsequent refinements) served as a continuous form of UAT. Each refinement to the reward shaping, training workflow, or bug fix was a direct response to observed agent behavior or system issues, ensuring that the system met the evolving expectations and requirements. For example, the discussions around agent stagnation and the subsequent need for a prolonged jump wrapper directly led to user-accepted improvements in agent behavior.

### Error-free and Error Handling

Significant effort was dedicated to developing a largely error-free system through meticulous coding practices and a modular design. Python's dynamic typing was managed with careful attention to data types and tensor shapes, particularly within the PyTorch framework. The project addressed and resolved several specific bugs and implemented robust error handling mechanisms. A critical environment setup error related to an `OverflowError` was encountered and, though fixed externally, underscored the importance of robust environment initialization. Issues with model and optimizer instantiation in the training script were corrected to ensure proper initialization and device placement. Redundant model loading and incorrect rendering in the evaluation script were fixed to ensure efficient model handling and accurate visual output. A `KeyError` in the training script was resolved by ensuring proper initialization of the information dictionary before access. `TypeError` instances in both the training and visualizer scripts were fixed by correctly passing the information dictionary to agent methods. A `ZeroDivisionError` in a testing script was addressed by ensuring the epsilon decay rate was a non-zero value. A `RuntimeError` in the base agent was fixed by ensuring state tensors were correctly normalized before being passed to the neural network. A recurring `ValueError` related to stepping in a done environment, primarily due to conflicts between environment wrappers and the visualizer resetting the main environment, was meticulously debugged and resolved by re-architecting the prolonged jump wrapper to be stateful and isolating the visualizer's environment instance. The use of dedicated configuration files also helped centralize hyperparameters, reducing errors from scattered magic numbers. While explicit `try-except` blocks for every possible runtime error were not extensively implemented, the iterative development and testing process, combined with detailed logging, allowed for rapid identification and resolution of issues as they arose. The system's modularity also meant that errors were often localized, simplifying the debugging process. This rigorous approach to testing and proactive bug-fixing ensured that the developed system was largely error-free in its core functionalities and capable of handling various operational scenarios, leading to a stable and reliable Deep Reinforcement Learning training and evaluation platform.

## 10. Conclusion

This academic project successfully developed and evaluated a Deep Reinforcement Learning agent for Super Mario Bros., fulfilling its primary objectives. We implemented and compared DQN, Double DQN, and Dueling DQN architectures, demonstrating their applicability to complex game environments. The project's modular design, built upon PyTorch, facilitated efficient development and experimentation.

Key achievements include the creation of a robust environment pipeline with custom wrappers for effective frame preprocessing and, most critically, an iteratively refined reward shaping mechanism. This reward shaping, particularly the introduction of penalties for stagnation and a wrapper to ensure prolonged jumps, proved instrumental in overcoming common DRL challenges such as local optima and inefficient exploration, enabling the agent to learn more human-like behaviors like performing high jumps and making genuine forward progress.

Furthermore, the comprehensive training pipeline, integrated with advanced logging, visualization tools (TensorBoard, GIF recordings), and a genetic algorithm for hyperparameter optimization, provided invaluable insights into the agent's learning dynamics. These tools were essential for debugging, performance analysis, and guiding the iterative refinement process. The system's ability to save and load models, coupled with dedicated evaluation and playback scripts, ensures reproducibility and ease of deployment.

While the agent demonstrated significant progress in navigating levels, future work could explore more advanced DRL algorithms (e.g., PPO, SAC), incorporate curiosity-driven exploration, or investigate transfer learning techniques to generalize across different Mario levels or even other platformer games. The project successfully laid a strong foundation for further research in applying DRL to classic video games.

## 11. References

Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., & Riedmiller, M. (2013). *Playing Atari with Deep Reinforcement Learning*. arXiv preprint arXiv:1312.05602. [https://arxiv.org/abs/1312.05602](https://arxiv.org/pdf/1312.05602)

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. *Nature*, *518*(7540), 529-533. [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)

Schaul, T., Quan, J., Antonoglou, I., & Silver, D. (2015). *Prioritized Experience Replay*. arXiv preprint arXiv:1511.05952. [https://arxiv.org/abs/1511.05952](https://arxiv.org/pdf/1511.05952)

Van Hasselt, H., Guez, A., & Silver, D. (2016). Deep reinforcement learning with double q-learning. In *Proceedings of the AAAI conference on artificial intelligence* (Vol. 30, No. 1). [https://ojs.aaai.org/index.php/AAAI/article/view/10295](https://ojs.aaai.org/index.php/AAAI/article/download/10295/10154)

Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., & Freitas, N. (2016). Dueling network architectures for deep reinforcement learning. In *International conference on machine learning* (pp. 1995-2003). PMLR. [http://proceedings.mlr.press/v48/wangf16.html](http://proceedings.mlr.press/v48/wangf16.pdf)

Ng, A. Y., Harada, D., & Russell, S. (1999). Policy invariance under reward shaping: Theory and applications. In *ICML* (Vol. 99, pp. 278-287). [http://rail.eecs.berkeley.edu/deeprlcoursesp17/docs/ng-thesis.pdf](http://rail.eecs.berkeley.edu/deeprlcoursesp17/docs/ng-thesis.pdf)
